# ASR_Model
*Whisper Hindi ASR Model at IIT Bombay Internship*

The Whisper Hindi ASR (Automatic Speech Recognition) model is a groundbreaking development that utilizes the KathBath dataset, a comprehensive collection of speech samples in Hindi. This model, trained on the KathBath dataset, incorporates cutting-edge deep learning techniques to accurately transcribe spoken Hindi into text. Its architecture leverages the power of neural networks to recognize and interpret the nuances of the Hindi language, including regional accents and dialects. Whisper is distinguished by its efficiency and accuracy, offering a dependable solution for converting spoken Hindi into written text across various applications, from transcription services to voice-enabled interfaces.

The KathBath dataset plays a pivotal role in training the Whisper model, providing a diverse array of speech samples sourced from different regions, covering a wide range of topics and conversational styles. This rich and varied dataset enables Whisper to generalize effectively across various real-world scenarios, ensuring robust performance in different contexts.

In addition to transcribing speech, ASR models like Whisper also calculate the Word Error Rate (WER) by comparing their output to a reference transcription. This metric quantifies accuracy, with lower WER values indicating better performance.

*Word Error Rate (WER)*

Word Error Rate (WER) is a critical metric used to evaluate the performance of automatic speech recognition (ASR) systems. It measures the disparity between the transcribed output generated by the ASR system and the reference or ground truth transcription. WER is calculated as the ratio of the total number of errors (insertions, deletions, and substitutions) required to align the transcribed text with the reference text, divided by the total number of words in the reference text. Lower WER values indicate higher accuracy, with a perfect score of 0 indicating an exact match between the transcribed and reference texts. WER serves as a widely used evaluation metric in the field of speech recognition, offering valuable insights into the performance and quality of ASR models across different languages and applications.

*Whisper Model Overview*

Whisper stands as a versatile speech recognition model, trained on a large and diverse dataset of audio samples. It serves as a multitasking model capable of performing various tasks including multilingual speech recognition, speech translation, and language identification.

A Transformer sequence-to-sequence model forms the backbone of Whisper, trained on a spectrum of speech processing tasks such as multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, enabling a single model to replace multiple stages of a traditional speech-processing pipeline. The multitask training format utilizes a set of special tokens serving as task specifiers or classification targets.

*Setup*

Python 3.8-3.11 and PyTorch 1.10.1 were utilized for training and testing the models, with compatibility expected across recent Python versions and PyTorch releases. The codebase depends on several Python packages, notably OpenAI's tiktoken for its fast tokenizer implementation. Installation or update of the Whisper package can be achieved with the following command:

bash
pip install -U openai-whisper


Alternatively, the latest commit from the repository along with its Python dependencies can be pulled and installed using the following command:

bash
pip install git+https://github.com/openai/whisper.git 


To update the package to the latest version of the repository, the following command can be used:

bash
pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git


Additionally, the command-line tool ffmpeg must be installed on the system, available from most package managers:

bash
# Ubuntu or Debian
sudo apt update && sudo apt install ffmpeg

# Arch Linux
sudo pacman -S ffmpeg

# MacOS using Homebrew
brew install ffmpeg

# Windows using Chocolatey
choco install ffmpeg

# Windows using Scoop
scoop install ffmpeg


This setup ensures a seamless implementation and evaluation of the Whisper ASR model, facilitating its deployment and usage in variousÂ environments.
